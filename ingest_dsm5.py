import os
import json
from typing import List
from tqdm import tqdm  # Bara de progres
from dotenv import load_dotenv

# LibrÄƒrii pentru PDF È™i Text Splitting
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ClienÈ›ii API
from supabase import create_client, Client
from openai import OpenAI

# ÃncarcÄƒ variabilele din .env
load_dotenv()

# --- CONFIGURARE ---
# Le citim din .env sau lÄƒsÄƒm string gol dacÄƒ nu existÄƒ
SUPABASE_URL = os.getenv("SUPABASE_URL", "")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY", "") 
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# Configurare Model (CRITIC: Trebuie sÄƒ fie acelaÈ™i ca Ã®n n8n)
MODEL_NAME = "text-embedding-3-small" 

def get_embedding(text: str, client: OpenAI) -> List[float]:
    """Trimite textul la OpenAI È™i primeÈ™te vectorul Ã®napoi."""
    text = text.replace("\n", " ")
    response = client.embeddings.create(input=[text], model=MODEL_NAME)
    return response.data[0].embedding

def main():
    # VerificÄƒri chei
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ EROARE: Lipsesc credenÈ›ialele Supabase (URL sau SERVICE_KEY) Ã®n fiÈ™ierul .env")
        return
    if not OPENAI_API_KEY:
        print("âŒ EROARE: LipseÈ™te OPENAI_API_KEY Ã®n fiÈ™ierul .env")
        return

    # IniÈ›ializare clienÈ›i
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        print(f"âŒ EROARE la iniÈ›ializare clienÈ›i: {e}")
        return

    pdf_path = os.path.join("data", "dsm5.pdf")
    
    if not os.path.exists(pdf_path):
        print(f"âŒ Nu gÄƒsesc fiÈ™ierul {pdf_path}. Te rog sÄƒ Ã®l pui Ã®n acest folder.")
        return

    print("ğŸ“– 1. Citesc PDF-ul...")
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        print(f"âœ… ÃncÄƒrcat {len(pages)} pagini.")
    except Exception as e:
        print(f"âŒ EROARE la citirea PDF-ului: {e}")
        return

    print("âœ‚ï¸  2. Tai textul Ã®n bucÄƒÈ›i (Chunking)...")
    # Chunk size 1000 caractere cu overlap 100 este standardul de aur pentru RAG
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(pages)
    print(f"âœ… Rezultat: {len(chunks)} bucÄƒÈ›i de text (chunks).")

    print(f"ğŸš€ 3. Generare Embeddings cu modelul '{MODEL_NAME}' È™i Upload Ã®n Supabase...")
    
    # ProcesÄƒm Ã®n loturi (batch-uri) de 50 pentru vitezÄƒ
    batch_size = 50
    total_chunks = len(chunks)

    for i in tqdm(range(0, total_chunks, batch_size)):
        batch = chunks[i : i + batch_size]
        rows_to_insert = []

        for doc in batch:
            content = doc.page_content
            # CurÄƒÈ›Äƒm caracterele nule care dau eroare Ã®n Postgres
            content = content.replace('\x00', '')
            
            # PÄƒstrÄƒm numÄƒrul paginii pentru citÄƒri!
            metadata = doc.metadata # ex: {'source': 'dsm5.pdf', 'page': 45}

            try:
                # GenerÄƒm embedding
                vector = get_embedding(content, openai_client)
                
                rows_to_insert.append({
                    "content": content,
                    "metadata": metadata,
                    "embedding": vector
                })
            except Exception as e:
                print(f"âš ï¸ Eroare la embedding pentru un chunk: {e}")

        # InserÄƒm lotul Ã®n Supabase
        if rows_to_insert:
            try:
                supabase.table("dsm5").insert(rows_to_insert).execute()
            except Exception as e:
                print(f"âŒ Eroare la upload Supabase: {e}")

    print("\nğŸ‰ GATA! Baza de date a fost populatÄƒ.")

if __name__ == "__main__":
    main()
